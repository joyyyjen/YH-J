<!DOCTYPE html>
<html>
    <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <link rel="shortcut icon" href="/img/dp.png" type="image/x-icon" />
    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-113303558-2', 'auto');
	
	ga('send', 'pageview');
}
</script>

    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-113303558-2', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


    
    <title>
      
      [2018]Topic Modeling - JoyyyJen
      
    </title>

    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    
    <link rel="stylesheet" type="text/css" href="/assets/css/normalize.css" />
    <link rel="stylesheet" type="text/css" href="/assets/css/icons.css" />
    <link rel="stylesheet" type="text/css" href="/assets/css/screen.css" />
    
    <link href="https://fonts.googleapis.com/css?family=Bree+Serif|Caveat|Lato:100,100i,300,300i,400,400i,700,700i|Source+Code+Pro:300,400,500,700" rel="stylesheet">
    

    
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
    <script type="text/javascript" src="/assets/bigfoot/dist/bigfoot.js"></script>
    <link rel="stylesheet" type="text/css" href="/assets/bigfoot/dist/bigfoot-number.css" />
    <script type="text/javascript">
        $.bigfoot();
    </script>
    
    
</head>

    <body class="page-template">
        <header class="main-header">
	<div class="main-header-content">
		<h1 class="blog-title">
			<a href="/">
				
           JoyyyJen
				
			</a>
		</h1>
		<h2 class="blog-description">四處瞧瞧，寫寫看看，就醉了</h2>
	</div>

	<div class="nav">
    
		
      <a class="nav- " role="presentation" href="/">Portfolio</a>
		
      <a class="nav- " role="presentation" href="/project/">Projects</a>
		
      <a class="nav- " role="presentation" href="/work/">Works</a>
		
		<a class="nav- " role="presentation" href="https://ajoy.me/">Blog</a>
	</div>
</header>

        
<main class="content" role="main">
  <article class="post page">
    <header class="post-header">
      <h2 class="post-title">[2018]Topic Modeling</h2>
    </header>
    <section class="post-content">
      <p><strong>Project Title</strong> :  Topic Modeling on Patient Review<br />
<strong>Skill</strong>:  python, corpus<br />
<strong>Toolkit</strong>: Gensim, NLTK<br />
<strong>Summary</strong> :</p>

<ul>
<li>Built topic model on RateMD database by applying the Latent Dirichlet Allocation model from Gensim library.<br /></li>
<li>Conducted and analyzed text cleaning procedures and lemmatization to find the optimal model<br />
<br /></li>
</ul>

<hr />

<p>Topic model is a type of statistical model for discovering the abstract &quot;topics&quot; in a collection of text documents.<br />
It is often used for discovery of hidden semantics structures in a text.</p>

<p>Latent Dirichlet Allocation (LDA) is an example of a topic model: a generative statistical model. It is used to classify text in a document to a particular topic.</p>

<h3 id="cleaning-procedure">Cleaning Procedure</h3>

<ul>
<li>text to lowercase<br /></li>
<li>stop words and punctuations removal<br /></li>
<li>stemming vs. lemmatization<br /></li>
<li>other data(image, tag, video, etc) cleaning steps<br />
<br /></li>
</ul>

<ol>
<li>converted corpus to lowercase can eliminate the number of unique words<br /></li>
<li>Stop words can cause high frequency as in Zipf’s Law. Punctuation is not necessary for text analyzing.<br /></li>
<li>Lemmatization might be a better case because stemming create words that might not be a real word in the dictionary. And can cause confusion when we manually label the topics.<br />
<br /></li>
</ol>

<h3 id="dictionary">Dictionary</h3>

<ul>
<li>Create a dictionary from the processed file containing the number of times a word appears in the training set<br /></li>
<li>Filter out tokens that are not that useful<br />
-- appear in less than x documents<br />
-- more than half of the documents (might be a stop word!)<br />
-- keep only y most frequent tokens<br />
<br /></li>
</ul>

<pre><code># Gensim doc2bow
dictionary = gensim.corpora.Dictionary(processed_docs)
dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)
bow_corpus = [dictionary.doc2bow(doc) for doc in proc_docs]
</code></pre>

<h3 id="models-lda">Models - LDA</h3>

<pre><code>#Set 1: number of topics (k = 10), number of passes (pass = 20), and number of iterations (iterations = 2000)
lda_model1 = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=20, workers=1,iterations=2000)

#Set 2: number of topics (k = 20), number of passes (pass = 20), and number of iterations (iterations = 2000)
lda_model2 = gensim.models.LdaMulticore(bow_corpus, num_topics=20, id2word=dictionary, passes=20, workers=1,iterations=2000)

</code></pre>

<h4 id="lemmatization">Lemmatization</h4>

<pre><code>wnl = WordNetLemmatizer()
process = []
for i in doc:
    pos = pos_tag([i])[0][1]
    if pos[0] == 'V': process.append(wnl.lemmatize(i,'v'))
    else: process.append(wnl.lemmatize(i))       

</code></pre>

<h3 id="output">Output</h3>

<p><strong>LDA-Set1 Without Lemmatization</strong><br />
<img src="/ldaset1.png" alt="img1" /><br />
<strong>LDA-Set2 Without Lemmatization</strong><br />
<img src="/ldaset2.png" alt="img2" /><br />
<strong>LDA-Set3 With Lemmatization</strong><br />
<img src="/ldaset3.png" alt="img2" /><br />
<strong>LDA-Set4 With Lemmatization</strong><br />
<img src="/ldaset4.png" alt="img2" /></p>

<h3 id="conclusion">Conclusion</h3>

<p>I would say LDA model with lemmatization for k = 10 generates better topics.<br />
From the generated top words in each topic for LDA with lemmatization, we will not see the same lemma. Lemmatization reduces the number of unique words by removing conjugation or inflections.</p>

<p>As we increase the number of topics, the models are likely to generate ambiguous topics with fussy semantics category such as ‘unknown*’ set2.</p>
    </section>
  </article>
</main>

        <footer class="site-footer">
  <section class="icon"> <a class="icon2-github" href="https://github.com/joyyyjen"></a> <a class="icon2-linkedin" href="https://www.linkedin.com/in/joyjen/"></a></section>
  <section class="copyright">&copy; 2020 JoyyyJen</section>
  <section class="poweredby"><a href="http://thedarkroast.com/arabica">Arabica</a> theme by Sean Lunsford. Published with <a href="https://gohugo.io">Hugo</a>.</section>
</footer>



    </body>
</html>
